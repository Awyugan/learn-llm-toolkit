# Andrew Ng and Chris Manning Discuss Natural Language Processing

## ref

- [youtube](https://www.youtube.com/watch?v=6w0Po83ZmjA&list=PLoROMvodv4rNS_gfs76OBp97bXIhYWt07&index=4)
- [Bilibili](https://www.bilibili.com/video/BV1Jh4y1e73S/?vd_source=05ecd5579b01bc45b90cd406c70a88e7)

## 正文

**吴恩达（Andrew Ng）**：大家好，我很高兴能和我的老朋友兼合作者克里斯·曼宁教授一起来到这里。克里斯有一个非常长且令人印象深刻的简历，但简单地说，他是斯坦福大学计算机科学教授，也是斯坦福人工智能实验室的主任。他还成为自然语言处理（NLP）领域被引用次数最多的研究人员。所以，很高兴和你在一起，克里斯。

**Chris Manning**：很高兴有机会和安德鲁聊天。

**吴恩达**：所以我们认识彼此合作很多年了，我一直认为你背景中一个有趣的部分是，即使今天您是NLP机器学习方面的杰出研究员，实际上您是从一个非常不同的领域开始的。如果您我没记错的话，您的博士学位是语言学，您正在研究语言的语法。那么您是如何从学习语法变成一名NLP研究员的呢？

**Chris Manning**：所以我当然可以告诉你这一点，但我还应该指出，我实际上仍然是一名语言学教授。我在斯坦福大学有一个联合约会。千载难逢的情况并不常见，我实际上仍然在教一些真正的东西。语言学以及计算机涉及自然语言处理。所以一开始，我对人类语言非常感兴趣——它们如何运作、人们如何理解它们、如何获得它们。所以我在人类语言中看到了这种吸引力。但这同样让我思考我们现在非常重视的想法，很多时候被认为是机器学习或计算思想。

**吴恩达**：所以我们讨论了很多关于NLP自然语言处理的问题。那么对于一些第一次接触机器学习的学习者来说，您能介绍一下什么是NLP吗？

**Chris Manning**：当然，绝对可以。所以NLP代表自然语言处理，另一个词是计算语言学，它是同一件事。自然语言处理实际上是一个奇怪的术语，对吧？所以这意味着我们正在用人类语言做事。所以你必须有这样的观念：你已经足够成为一名计算机科学家，当你说语言时，你用你的大脑编程语言来思考。因此，你需要说自然语言来表示你正在谈论人类使用的语言图像。所以总体而言，自然语言处理正在用人类语言做任何智能的事情。所以从某种意义上来说，这可以分解为理解人类语言、产生人类语言、习得人类语言，尽管人们也经常从不同的应用角度来思考它。那么你可能会考虑诸如机器翻译或回答问题或生成广告文案或摘要。人们在处理许多不同的任务时都带着特定的目标，而你用人类语言来做事。还有大量的自然语言处理，因为我们人类世界中的很多工作都是经过处理和处理的人类语言材料的形式传播。

**吴恩达**：所以，因为所有这些应用程序甚至网络搜索，对吧？

**Chris Manning**：是的，没错。我们大多数人都使用NLP。吴恩达：一天很多很多次。Chris Manning：你是对的，在某种意义上，自然语言最大的应用是网络搜索，对吗？这确实是最重要的，我是说，传统上，这是一种简单的，对吧？在过去的美好时光里，你知道，有各种各样的等待因素等等，但主要是匹配关键字，然后是搜索词，然后是有关页面质量的一些因素。它真的不像是语言理解，但多年来情况确实在发生变化。所以这些日子，如果你向搜索引擎提出问题，它通常会给你一个答案框，其中提取了一段文本并将其认为的答案以粗体或颜色或类似的方式显示。这就是回答问题的任务，然后它真的是一个自然语言理解任务。

**吴恩达**：是的，是的，我觉得除了网络搜索之外，也许还有一个重要的搜索，即使我们要访问在线购物网站或电影网站并输入我们想要的内容并进行比大型搜索引擎小得多的网站上的网站搜索。它还越来越多地使用复杂的NLP算法，并且它也创造了相当多的价值。也许对你来说这不是真正的NLP，但它看起来仍然很有价值。

**Chris Manning**：我同意，这非常有价值。任何带有搜索的电子商务网站都会存在很多有趣的问题，非常困难的问题实际上当人们描述他们想要的商品种类时。你需要尝试将其与可用的产品相匹配，事实证明这根本不是一个简单的问题。

**吴恩达**：是的，确实如此。所以在过去，我不知道，几十年来，NLP已经从更多基于规则的技术经历了重大转变，您刚才提到要更普遍地使用真正的机器学习。所以你是领导部分工作的人之一，看到了每一步，你正在创造一些事情发生的根源，您能简单介绍一下这个过程以及您所看到的情况吗？

**Chris Manning**：当然，绝对。是的，所以当我开始作为本科生和研究生时，实际上大部分自然语言处理都是手工完成的系统，不同地使用规则和推理程序来进行尝试和推理的系统建立一条路径并理解一段文本。但只有当大量数字文本和语音开始变得可用时，看起来确实有这种不同的方式，相反我们可以开始计算人类语言的统计数据，材料和构建机器学习模型。所以我要做的第一件事，大概是20世纪90年代中后期。所以我开始做大量研究的第一个领域发表论文并广为人知正在构建早期我们通常称为统计自然语言处理的东西。但它后来并入了人工智能的一般概率方法和机器学习。这么说吧，我们就这样度过了大约2010年。大概就在那时，人们对深度学习使用大型人工神经网络开始产生新的兴趣。对于我对此的兴趣，我真的要感谢安德鲁，因为在这个阶段，安德鲁仍然在斯坦福大学全职工作，他在我隔壁的办公室里，他对新的东西感到非常兴奋，我猜是深度学习领域正在发生的事情。任何走进他办公室的人，他都会告诉他们，哦，这太令人兴奋了神经网络现在正在发生什么，你必须开始关注它。这确实是让我变得漂亮的动力很早就开始研究神经网络中的事物。事实上，我之前已经看过一些，所以当我还是这里的研究生时，实际上，大卫·鲁梅尔哈特 (David Rumelhart) 在斯坦福大学心理学专业，我上过他的神经网络课程。所以，我看到了其中的一些内容，但实际上并不是我自己的研究所涉及的内容。

**吴恩达**：我不知道，谢谢，是的。然后我们一起监督一些学生。我很想听听深度学习和NLP的兴起，自从您进入该领域以来，您看到了什么？

**Chris Manning**：是的，从2010年开始，是的，我，学生们，开始写第一篇论文针对NLP会议的深度学习。当你尝试做新事情时总是很困难。我们的经历与15年前左右的人们开始尝试进行统计NLP时的经历完全相同。由于既定的做事方式，很难推出新的想法。所以我们的一些第一篇论文确实被会议拒绝，而是出现在机器学习会议或深度学习研讨会，但很快情况就开始发生变化，人们对神经网络的想法非常感兴趣。但我感觉神经网络时代开始了实际上大约在2010年，它自我分裂为两部分，因为对于第一个时期，比如说，基本上是到2018年。我们在为各种任务构建神经网络方面取得了巨大成功。我们构建它们是为了进行句法分析和情感分析。还有什么？问答。但这有点像我们在做以前做过的同样的事情其他类型的机器学习模型，除了我们现在有了更好的机器学习模型。我们没有训练逻辑回归或支持向量机，而是在进行相同类型的情感分析任务，但现在我们用神经网络来完成它。所以我认为现在回想起来，从某种意义上说，更大的变化发生在2018年左右。因为那时，嗯，我们可以从大量的人类语言材料开始建立大型的自我监督模型。这就是当时的模型，比如BERT和GPT及其后继模型。他们可以从单词预测中获取大量的文本，揭示了人类语言的惊人知识。我认为现在回想起来，这很可能会被视为更大的切入点，在这个切入点上，做事的方式确实发生了变化。

**吴恩达**：是的，我认为大型语言模型从大量数据中学习是一种趋势。我想即使在这之前，你的一篇研究论文确实让我大吃一惊，那是一篇手套论文。因为通过词嵌入，您可以使用神经网络学习向量数来表示单词。这对我来说真是令人兴奋。然后你所做的手套工作确实清理了数学，使它变得如此简单得多。然后我记得我说过，要做的就是这些。然后你就可以了解到这些非常详细的内容计算机的表示学习单词含义的细微差别。

**Chris Manning**：当然。所以我应该给予别人一点信任。其他人也研究过一些类似的想法，包括Ronan Collobert、Jason Weston、托马斯·米科洛夫 (Tomas Mikolov) 和谷歌的同事。但手套词向量是非常突出的词向量系统之一。所以这些词向量已经做到了。是的，你是对的，请说明我们使用自我监督学习的想法只是获取了大量的文本。然后我们可以构建这些知道大量信息的模型关于词语的含义。这仍然是我每年都会向人们展示的东西在我的NLP课程的第一堂课中。因为它很简单，但实际上效果却出奇的好。您可以进行这种简单的建模，尝试根据上下文中的单词来预测单词，并且只需通过运行数学学习来进行这些预测。好吧，你学到了所有这些关于单词含义和你可以做这些非常漂亮的类似词义的图案，或者铅笔是绘画，画笔也是如此。它会说画画吧？它已经显示出很多成功的学习成果。所以这就是后来发展到下一个的先驱阶段与BERT和GPT等领域，它不仅仅是单个单词的含义。而是整个文本和上下文的含义。

**吴恩达**：但是整个文本和上下文的含义。然后是2018年，也许这又是一个拐点，之后发生了什么？

**Chris Manning**：是的。所以在2018年，这就是关键时刻，确实发生了两件事。有一件事是，人们或者确实在2017年就开发出了这种新架构。它在现代并行GPU上的可扩展性要高得多。这就是变压器架构。第二部分可能是人们重新发现了，因为我使用了与手套模型相同的技巧，如果你有任务是在给定的上下文中预测一个单词。要么是它两边的上下文，要么是前面的话表明，这确实是一项令人惊奇的学习任务。这让很多人感到惊讶。很多时候你会看到人们在讨论中说一些贬低的话，这没什么有趣的事情发生。它所做的只是通过统计来预测哪个单词是最有可能出现在前面的单词之后。我认为真正有趣的是，这是事实，但事实并非如此。因为 yes，任务是根据前面的单词预测下一个单词。但真正有趣的是，如果你想尽可能好地完成这项任务，那么它实际上有助于理解整个句子的其余部分，知道谁对谁做了什么以及句子中的内容。但不仅如此，它还有助于理解世界，因为如果你的文字是按照斐济使用的货币的方式进行的，那么你需要有一些世界知识才能知道正确的答案是什么。如此擅长做到这一点的优秀模特，学习遵循句子的结构及其含义，了解世界的全部事实，以便他们能够预测。因此，这变成了有时被称为人工智能完成的任务，对吗？你真的需要没有什么实际上不能用来回答这个词的下一个含义，对吧？你可以在世界杯半决赛中，球队……你需要了解一些关于足球的知识才能给出正确的答案。

**吴恩达**：人工智能完整是一个有趣的概念，对吧？这个想法是你可以解决这一个问题，你可以解决人工智能中的所有问题还是有点从计算理论来类比NP完全问题。你怎么认为？你认为预测下一个单词是人工智能完成的吗？

**Chris Manning**：我认为这并不完全正确，因为我认为还有其他人类设法解决的事情。有些人在数学或数学方面有聪明的洞察力，有些人正在寻找更重要的东西三维现实世界的难题，类似于弄清楚如何做一些机械或类似的事情。这不是语言问题。但另一方面，我认为语言变得比一些人想象的更接近普遍性，因为我们生活在这个3D世界中。并用我们的身体和感受来运作周围的其他生物和文物。你可能会想，好吧，其中根本没有太多是用语言来表达的。但实际上，我们思考、谈论、写下的所有这些东西都是用语言表达的。我们可以用语言描述事物之间的相对位置。所以，世界其他地区的数量惊人，都在反思中在语言中。因此你也在学习所有这些。当你学习语言的使用时。

**吴恩达**：你可以了解很多事情的某一方面，即使像你如何骑自行车这样的事情并不真正完美。

**Chris Manning**：你并没有真正学会如何骑自行车，但你学到了其中涉及的一些方面，你需要平衡。你必须把脚放在踏板上并推动它们等等。是的。

**吴恩达**：所以随着NLP的这一趋势，最近几年最新的语言模型非常令人兴奋。您对这一切将走向何方有何想法？

**Chris Manning**：嗯，是的，所以这真是令人惊讶成功且令人兴奋，对吗？所以我们还没有真正解释所有细节，对吧？所以学习这些大型语言模型的第一阶段的任务只是预测下一个单词。你对一大段文本执行了数十亿次。看哪，你得到了这个大型神经网络，这是一个非常有用的适用于各种自然语言处理任务的工件。但然后你实际上仍然必须用它做点什么，无论是回答问题、总结还是检测社交媒体中的有毒内容等等。到那时，你有选择用它来做一些事情。传统的答案是你有一项特定的任务，比如说它正在检测社交媒体中的有毒评论。你会为此获取一些监督数据，然后你可以微调语言模型来回答该分类任务。但是这个庞大的自我监督基地对你有很大帮助，因为这意味着该模型拥有大量的语言知识，并且可以非常快速地进行概括。与过去标准的监督学习不同的是，如果你给我10,000个带标签的示例，我也许能为你制作一个还算不错的模型。但如果你给我50,000个带标签的例子，那就好多了。它有点把它变成了这个世界。好吧，如果你给我100个带标签的例子并且我对大型语言模型进行微调，我将能够做得很好，比我在旧世界中使用50,000个示例所能做的更好。现在有一些更令人兴奋的作品，甚至超越了这些，现在，好吧，也许你实际上根本不需要微调模型。所以人们使用方法做了很多工作，有时也称为作为提示或说明，你可以简单地用自然语言，也许有例子，也许有明确的指令，只需告诉模型你想让它做什么，它就会这样做，即使作为某人他在自然语言处理领域工作了30年。我的意思是，它的效果真的让我大吃一惊，我想十年前我并没有想到现在我们可以直接告诉模型，我希望你总结一下这一点这里有一段文字，然后他们会总结它。我认为这太不可思议了。

**吴恩达**：我们正处于这个非常激动人心的时代，许多新的自然语言功能正在不断涌现。我认为在接下来的几年里毫无疑问，随着人们解决不同的事情，未来是非常光明的不同的做事方式和人们开始应用于不同的应用领域。随着最近的技术发展而释放的能力。技术上始终存在一个问题，即曲线是否继续急剧上升，或者是否还有一些新的东西，我们必须去探索如何去做。

**Chris Manning**：我认为两者兼而有之，我认为这将是未来的方式，但我也认为目前人们正在做很多修改和改写，以尝试和让事情变得更好，如果运气好的话，再过几年的发展就会开始消失。我的意思是，思考差异的一种方法是进行比较，提供的语音帮助或虚拟帮助手机扬声器设备如亚马逊、Alexa这些天，对吗？我的意思是，我认为我们所有人都有过这样的经历：这些设备并不总是很棒，但是如果你知道正确的表达方式，它就会有所作为。但如果你使用了错误的措辞，它就不会，与人类的区别大体上是这样的，你不必考虑这一点。你可以说你想说的话，无论你选择什么词，他们都会认为对方知道相同的语言，等等都会理解你并做你想做的事。我认为并且希望我们将开始看到同样的进展，这些模型目前正在摆弄特定的措辞，您使用的模型可以对其工作效果产生很大的影响。但希望几年后，这不会是真的，你将能够使用不同的措辞，但它仍然有效。但基本的想法是我们正在进入这个实际上是人类语言的时代，将能够用作指令语言来告诉您的计算机要做什么。所以，不必使用菜单和单选按钮之类的东西或编写Python代码，而不是那些你可以说你想要在计算机中做什么的事情。我认为这个时代正在我们面前展开，它将继续建设和发展，这将带来巨大的变革。

**吴恩达**：感觉已经走了很长一段路，但还有更多的路要走。

**Chris Manning**：是的，绝对是。在NLP技术的发展中，唯一我想问你的问题，我怀疑你和我可能有不同的看法。但在过去的几十年里，趋势是减少对基于规则的工程的依赖，而更多地依赖数据的机器学习。有时，大量数据展望未来。您认为手工编码约束或其他约束的混合在哪里？显式约束与让我们建立一个神经网络并向其输入大量数据，您认为这种平衡会落在哪里？

**Chris Manning**：我认为毫无疑问，利用学习数据是前进的方向，也是我们将继续做的事情。但我认为具有更多结构、更多归纳偏差的模型仍然有空间，具有某种利用语言本质的基础。所以近年来，非常成功的模型是变压器神经网络，本质上就是这个巨大的关联机器。所以它只会吸收来自任何地方的关联。你用一切来预测任何事情，一遍又一遍地做，你就会得到你想要的任何东西。你知道，这是非常非常成功的，但它在拥有大量数据的领域取得了令人难以置信的成功。是的，这些大型语言模型的转换器模型现在正在数百亿个文本单词上进行训练。当我开始从事统计自然语言处理时。一些传统语言学家曾经抱怨过这个事实，我正在从Newswire的3000万字中收集统计数据。并建立一个预测模型，认为这不是语言学的目的。我觉得我有一个非常好的答案，即一个正在学习语言的人类孩子。他们实际上接触到了超过3000万字的数据。但那种数据量，所以我们的一些数据量使用的是完全合理的数据量。并不是试图模拟人类语言习得，而是思考我们如何从大量数据中学习语言。但这些现代变压器现在已经在使用至少两个数量级，更多的数据。大多数人都会思考如何让事情更上一层楼，是用更多，让它提高三个数量级。从某种意义上说，扩大战略规模非常有效。所以，我不会责怪任何人说，让我们把它再扩大一个数量级，看看我们能做些什么令人惊奇的事情。但这也表明人类学习在能够提取更多信息方面要好得多，从相当有限的数据中获取信息。到那时，你可以有各种假设。但我认为人类学习的假设是合理的，某种程度上是针对世界结构的。以及它在世界上看到的事物，那使得它能够从更少的数据中更快地学习。

**吴恩达（Andrew Ng）**：好的，我同意你的观点。我认为更好的学习算法，我们当前的机器学习算法效率低得多，或者数据的利用效率低得多。因此，这里的数据比任何孩子都多。然后我想改进的学习算法是否来自类似语言的规则，或者是否只是工程师设计更高效的变压器版本或之后的任何产品。我认为会的。

**Chris Manning**：我怀疑这会是传统的。我不认为人们会明确地将传统的语言规则纳入系统。我认为这不是前进的方向。另一方面，我认为我们开始看到的是模型，像这些变压器模型实际上是在发现语言本身的结构，对吗？所以英语对人类语言的广泛影响是在动词之前有主语，在动词之后有宾语。而在日语中，句子末尾的动词以及主语和宾语通常按顺序排列在句子之前。但实际上 Transformer 模型可能以其他顺序学习这些事实。你可以询问他们，发现即使他们从未被明确告知过主体和客体，他们也知道这些概念。所以我认为他们还发现了很多关于语言使用和上下文的其他内容，词语的意义和意义，以及什么是、什么不是，令人不快的语言。但他们正在学习的部分内容与语言学家的结构相同，已经被布置为不同人类语言的结构。

**吴恩达**：就好像几十年来，语言学家发现了某些东西。通过对数十亿个单词的训练，变形金刚正在发现与语言学家在人类身上发现的相同的东西。那很酷。所以这一切都是 NLP 领域令人兴奋的进步，由机器学习和其他事物驱动。对于进入该领域、进入机器学习、人工智能或自然语言处理的人，发生了很多事情。对于想要进入机器学习领域的人，您有什么建议？

**Chris Manning**：嗯，现在是闯入的好时机。我认为毫无疑问我们仍处于早期阶段，看到这种新方法的影响，有效地软件计算机科学正在被重新发明，基于更多地使用机器学习。以及由此产生的各种其他事情。更普遍地说，跨行业，有很多机会更多自动化。为我或更多地利用人类语言材料的解释，在其他领域如视觉和机器人等，也有类似的事情。所以，有很多可能性。因此，在这一点上，显然有很多事情要做，因为你想要获得某种良好的基础，对吧。了解机器学习的一些核心技术方法，了解如何从数据构建模型的思想。查看损失，进行培训诊断错误，所有这些核心事情。这对于自然语言处理尤其有用，其中一些技能是完全相关的。但也有一些常用的特殊模型，包括我们今天谈论很多的变压器。你绝对应该了解变压器，事实上，它们越来越多地用于机器学习的各个其他部分以及视觉、生物信息学，甚至机器人技术现在也使用变压器。但除此之外，我认为了解一些有关人类语言和所涉及问题的本质也很有用。因为即使人们不会直接将人类语言规则输入到他们的计算系统中。对语言中发生的事情的敏感性，需要注意什么以及您可能想要建模什么，这仍然是一项有用的技能。

**吴恩达**：然后就学习基础知识而言，了解这些概念。你是从语言学背景进入人工智能领域的，并且我们现在看到各行各业的人们都想开始从事人工智能工作。您对一个人应该做的准备有什么想法，或者您对如何从计算机科学或人工智能以外的其他领域开始有什么想法。

**Chris Manning**：所以你可以从很多地方出发，并以不同的方式穿越。我们看到很多人这样做，他们是人，他们从不同的领域开始，无论是化学、物理学，甚至更深入的领域。人们有历史，无论什么都开始关注机器学习。我认为答案有两个层次。一级答案是令人惊奇的转变之一是现在有这些非常好的软件包，用你的网络模型做事。这个软件真的很容易使用。你实际上不需要了解很多高技术含量的东西。你需要对这个想法有某种高层次的概念，机器学习是什么。我该如何训练模型，我应该看什么以及打印出来的数字来看看它是否有效，对吧。但你实际上并不需要拥有更高的学位才能构建这些模型。我的意思是，事实上我们看到的是，很多高中生正在这样做，因为这实际上是如果你有一些基本的计算机技能和一点编程技能，你可以学会并做。它比之前的许多东西更容易访问，无论是人工智能之外的人工智能还是其他领域，例如操作系统或安全性。但如果你想达到比这更深的层次，并且实际上想了解更多正在发生的事情。我认为如果你没有一定的数学基础，你就无法真正实现这一目标，就像深度学习最终一样基于微积分，你需要优化函数。如果你没有任何这方面的背景，我认为这在某种程度上最终会演变成一场战争。吴恩达：机器学习和数据科学中的数学。它确实对我们所做的一些工作很有用。Chris Manning：是的。所以我认为在某种程度上，如果你主修历史或心理学的非数学部分，我实际上有一个好朋友，是的，他在研究生院学习了微积分，因为他是一名心理学家，他以前从未这样做过。并决定开始学习这些新型模型，并决定现​​在去参加牛课程还为时不晚。他确实这么做了，对吧。所以你确实需要了解一些东西，但对于很多人来说，如果他们以前见过一些这样的东西，即使你有点生疏。我认为你可以回到状态，这并不重要，你作为本科生还没有做过人工智能或机器学习之类的事情，你可以真正开始学习如何构建这些模型并做事，这真的是我自己的故事，对吧？尽管他们现在让我在斯坦福大学工程学院就读，但我的背景并不是工程师。我的博士学位是语言学，我基本上是矢量化的，除了具备一定的数学和语言学知识之外，了解一些编程可以更好地构建人工智能模型。

**吴恩达**：让我们仔细研究一下。您认为现在可用的改进库和抽象像编码框架，如张量流或 Pytorch？您认为这会减少了解微积分的需要吗？因为，我已经有一段时间没有真正采用导数来实现或创建新的神经网络架构了，因为自动微分。

**Chris Manning**：是的，我的意思是，绝对如此。我的意思是，所以在早期，当我们在 2010 年到 2015 年做事情时，对吗？对于我们构建的每个模型，我们都会手动计算出导数，然后编写一些代码以及其他任何内容。有时是 Python，但有时可能是 Java 或 C [笑] 来计算这些导数，检查我们是否正确等等，这些天你实际上不需要知道任何这些来构建深度学习模型。我的意思是，这实际上是我正在思考的事情，甚至我一直在思考我自己教授的深度学习课程的自然语言处理。一开始，我们仍然要进行矩阵演算和确保人们了解雅可比行列式和类似的东西，他们了解反向传播、深度学习正在做什么。但有某种事情意味着我们只是让他们受苦两周。这有点像新兵训练营之类的让他们受苦的事情。然后我们说，但是你用 pytorch 完成剩下的课程，他们似乎再也不需要知道这些了，对吧？始终存在一个问题，即您想要在技术基础上走多深，正确的。你可以继续走下去，对吗？就像 2020 年代的计算机科学家需要理解的那样，电子器件和晶体管或者 CPU 中发生的情况。嗯，这很复杂，我的意思是，从各个方面来说，了解其中一些东西是有帮助的。我的意思是，我知道 Andrew，您是将机器学习应用到 GPU 上的先驱之一，而且，有点意味着你必须对存在这种新硬件有一定的感觉。它具有并行性的一些属性，这意味着它可能能够做一些令人兴奋的事情。因此，拥有更广泛的知识和理解是很有用的，有时有些东西会出现问题，如果你有更深入的知识，你就能理解它为什么坏了。但还有另一种意义，大多数人必须接受一些信任的事情，你可以在完全不了解微积分的情况下完成当今神经网络建模中想做的大部分事情。

**吴恩达**：是的，我认为这是一个很好的观点。我觉得有时抽象的可靠性决定了你需要多长时间去修复损坏的东西。所以我，实际上我对量子物理的理解非常薄弱。我勉强理解它。所以你可能会说，我不明白计算机是如何工作的，因为晶体管是在量子物理学中构建的。但幸运的是，如果晶体管出了问题，我从来不需要花很大力气去修复它。

**Chris Manning**：我认为有点难以修复。

**吴恩达**：所以我认为或者另一个例子，排序函数，用于排序的库，有时它们实际上不起作用，对吧，交换内存或其他什么。到那时，如果您真正了解排序函数的工作原理，您就可以进去修复它。但有时如果我们有抽象、库，API 足够可靠，那么它对这些抽象很好，从而减少了理解发生在我身上的一些事情的需要。所以这是一个令人兴奋的世界。感觉就像我们有巨人建立在巨人的肩膀上，这些事情确实每个月都变得更加复杂和令人兴奋。Chris Manning：是的，绝对是。

**吴恩达**：所以谢谢克里斯，那真的很有趣，也很鼓舞人心，我希望每个观看这次听证会的人都能了解克里斯自己的旅程成为一名计算机科学家。并成为领先的，也许是领先的 NLP 计算机科学家以及所有 NLP 领域目前正在开展这项令人兴奋的工作。我希望这也能激励您跃跃欲试并尝试一下。我们的社区还有很多工作需要共同完成，所以我认为我们中越多的人在这方面努力，世界就会变得更好。所以非常感谢，克里斯。It was really great having you here.

**Chris Manning**：Thanks a lot, Andrew. It's been fun chatting.
